# ── Atlas Cortex Environment Configuration ───────────────────────
# Copy to .env and customize. All values shown are defaults.

# ── General ──────────────────────────────────────────────────────
TZ=America/New_York
PUID=99
PGID=100

# ── Atlas Cortex Server ──────────────────────────────────────────
CORTEX_PORT=5100
# ADMIN_USERNAME=admin
# ADMIN_PASSWORD=atlas-admin

# ── LLM Backend (Ollama on primary GPU, port 11434) ─────────────
LLM_PROVIDER=ollama
LLM_URL=http://localhost:11434
# LLM_API_KEY=           # Only needed for OpenAI/Anthropic providers
MODEL_FAST=qwen2.5:7b
MODEL_THINKING=qwen3:30b-a3b
MODEL_EMBEDDING=nomic-embed-text

# Ollama LLM container image:
#   CPU:       ollama/ollama:latest
#   AMD ROCm:  ollama/ollama:rocm
OLLAMA_LLM_IMAGE=ollama/ollama:latest

# ── TTS (Orpheus primary on voice GPU, port 11435) ──────────────
TTS_PROVIDER=orpheus
ORPHEUS_URL=http://localhost:11435
ORPHEUS_MODEL=legraphista/Orpheus:3b-ft-q8
# ORPHEUS_FASTAPI_URL=    # Use Lex-au/Orpheus-FastAPI instead of Ollama

# Ollama Voice container image (for second GPU):
OLLAMA_VOICE_IMAGE=ollama/ollama:latest

# ── TTS Fallback (Piper CPU, port 10200) ─────────────────────────
PIPER_HOST=localhost
PIPER_PORT=10200
PIPER_VOICE=en_US-lessac-medium
PIPER_LENGTH=1.0
PIPER_NOISE=0.667
PIPER_NOISEW=0.333
PIPER_SPEAKER=0
PIPER_PROCS=1

# ── STT (Faster-Whisper, port 10300) ─────────────────────────────
STT_HOST=localhost
STT_PORT=10300
WHISPER_MODEL=distil-large-v3
WHISPER_BEAM=1
WHISPER_LANG=en

# Keep models loaded forever in multi-GPU mode (no unload latency)
OLLAMA_KEEP_ALIVE=0

# ── GPU Configuration ───────────────────────────────────────────
# GPU assignment per plan:
#   GPU 0 (largest VRAM)  → LLM        (atlas-ollama,       port 11434)
#   GPU 1 (second GPU)    → Voice/TTS  (atlas-ollama-voice, port 11435)
#
# For GPU support, use the appropriate override file:
#   AMD+Intel: docker compose -f docker-compose.yml -f docker-compose.gpu-amd.yml up -d
#   Intel:     docker compose -f docker-compose.yml -f docker-compose.gpu-intel.yml up -d
#   NVIDIA:    docker compose -f docker-compose.yml -f docker-compose.gpu-nvidia.yml up -d
