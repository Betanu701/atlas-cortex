# ── Atlas Cortex Environment Configuration ───────────────────────
# Copy to .env and customize. All values shown are defaults.

# ── General ──────────────────────────────────────────────────────
TZ=America/New_York
PUID=99
PGID=100

# ── Atlas Cortex Server ──────────────────────────────────────────
CORTEX_PORT=5100
# ADMIN_USERNAME=admin
# ADMIN_PASSWORD=atlas-admin

# ── LLM Backend (Ollama on primary GPU, port 11434) ─────────────
LLM_PROVIDER=ollama
LLM_URL=http://localhost:11434
# LLM_API_KEY=           # Only needed for OpenAI/Anthropic providers
MODEL_FAST=qwen2.5:7b
MODEL_THINKING=qwen3:30b-a3b
MODEL_EMBEDDING=nomic-embed-text

# Ollama LLM container image:
#   CPU:       ollama/ollama:latest
#   AMD ROCm:  ollama/ollama:rocm
OLLAMA_LLM_IMAGE=ollama/ollama:latest

# ── TTS (Orpheus primary via FastAPI, port 5005) ─────────────────
TTS_PROVIDER=orpheus
ORPHEUS_FASTAPI_URL=http://localhost:5005
ORPHEUS_MODEL_NAME=Orpheus-3b-FT-Q4_K_M.gguf
# Also available: Orpheus-3b-FT-Q8_0.gguf (higher quality, ~45% slower)
ORPHEUS_MAX_TOKENS=8192
ORPHEUS_TEMPERATURE=0.6
ORPHEUS_TOP_P=0.9
ORPHEUS_API_TIMEOUT=120

# Orpheus-FastAPI container (built from Betanu701/Orpheus-FastAPI v1.0.0):
#   atlas-orpheus      → SNAC decoder + API (port 5005)
#   atlas-llama-voice  → llama.cpp inference (port 5006, internal)
# To build from a local clone (e.g. for development):
# ORPHEUS_FASTAPI_BUILD_CTX=/path/to/local/Orpheus-FastAPI

# ── TTS Fallback (Piper CPU, port 10200) ─────────────────────────
PIPER_HOST=localhost
PIPER_PORT=10200
PIPER_VOICE=en_US-lessac-medium
PIPER_LENGTH=1.0
PIPER_NOISE=0.667
PIPER_NOISEW=0.333
PIPER_SPEAKER=0
PIPER_PROCS=1

# ── STT (whisper.cpp Vulkan, port 10300) ──────────────────────────
# Backend: "whisper_cpp" (HTTP API) or "wyoming" (Wyoming protocol)
STT_BACKEND=whisper_cpp
STT_HOST=localhost
STT_PORT=10300
WHISPER_MODEL=large-v3-turbo-q5_0
WHISPER_BEAM=5
WHISPER_LANG=en
# WHISPER_VK_DEVICE=1  # Set in GPU override (0=first GPU, 1=second)

# Keep models loaded forever in multi-GPU mode (no unload latency)
OLLAMA_KEEP_ALIVE=0

# ── GPU Configuration ───────────────────────────────────────────
# GPU assignment per plan:
#   GPU 0 (largest VRAM)  → LLM         (atlas-ollama,       port 11434)
#   GPU 1 (second GPU)    → Voice/TTS   (atlas-orpheus,      port 5005)
#                                        (atlas-llama-voice,  port 5006)
#
# For GPU support, use the appropriate override file:
#   AMD+Intel: docker compose -f docker-compose.yml -f docker-compose.gpu-amd.yml up -d
#   Intel:     docker compose -f docker-compose.yml -f docker-compose.gpu-intel.yml up -d
#   NVIDIA:    docker compose -f docker-compose.yml -f docker-compose.gpu-nvidia.yml up -d
